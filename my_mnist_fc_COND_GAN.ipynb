{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils import mnist, plot_graphs, plot_mnist, to_onehot\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'FC_COND_GAN_results'\n",
    "fixed_folder = root_folder + '/Fixed_results/'\n",
    "recon_folder = root_folder + '/Recon_results/'\n",
    "\n",
    "if os.path.isdir(root_folder):\n",
    "    !rm -r $root_folder\n",
    "os.mkdir(root_folder)\n",
    "os.mkdir(fixed_folder)\n",
    "os.mkdir(recon_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_tanh = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "                lambda x: x.to(device)\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-f7d5ddff0a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfixed_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#z_label = to_onehot(np.random.randint(0, 10, (batch_size)), 10, device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfixed_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfixed_z_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/playground/CAAE/utils.py\u001b[0m in \u001b[0;36mto_onehot\u001b[0;34m(x, n, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "lr = 0.0001\n",
    "prior_size = 100\n",
    "train_epoch = 1000\n",
    "batch_size = 250\n",
    "fixed_z = torch.randn((50, 100))\n",
    "#z_label = to_onehot(np.random.randint(0, 10, (batch_size)), 10, device)\n",
    "fixed_label = to_onehot(fixed_label[:100], 10).to(device)\n",
    "print(fixed_label)\n",
    "fixed_z_1 = torch.cat((fixed_z, fixed_label), 1)\n",
    "print(fixed_z_1.shape)\n",
    "train_loader, valid_loader, test_loader = mnist(batch_size=batch_size, valid=10000, transform=mnist_tanh)\n",
    "#fixed_z = torch.randn((10, prior_size)).repeat((1,10)).view(-1, prior_size).to(device)\n",
    "fixed_z_label = to_onehot(torch.tensor(list(range(10))).repeat((10)), 10).to(device)\n",
    "fixed_data, fixed_label = next(iter(test_loader))\n",
    "fixed_data = fixed_data[:100].to(device)\n",
    "fixed_label = to_onehot(fixed_label[:100], 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 1, 28, 28])\n",
      "torch.Size([250, 784])\n",
      "tensor([2, 5, 8, 3, 8, 5, 8, 4, 0, 3, 0, 1, 7, 8, 1, 5, 1, 9, 8, 4, 7, 3, 4, 4,\n",
      "        6, 4, 6, 6, 5, 8, 6, 0, 5, 6, 4, 5, 7, 3, 6, 9, 6, 8, 7, 8, 3, 6, 2, 9,\n",
      "        9, 4, 5, 7, 5, 7, 0, 8, 1, 8, 7, 3, 3, 5, 9, 1, 5, 9, 8, 4, 6, 6, 2, 4,\n",
      "        9, 3, 5, 6, 4, 1, 3, 9, 2, 8, 5, 8, 2, 0, 6, 2, 3, 9, 0, 5, 4, 2, 4, 7,\n",
      "        5, 2, 3, 4, 5, 3, 0, 0, 6, 4, 9, 1, 0, 4, 8, 2, 3, 7, 1, 7, 7, 4, 8, 7,\n",
      "        1, 7, 5, 2, 3, 0, 2, 3, 3, 8, 3, 3, 0, 9, 0, 9, 1, 5, 1, 2, 4, 0, 1, 1,\n",
      "        8, 2, 4, 8, 4, 2, 0, 3, 8, 3, 1, 9, 7, 8, 1, 0, 1, 2, 9, 8, 8, 8, 7, 5,\n",
      "        2, 7, 3, 2, 9, 2, 2, 4, 5, 3, 3, 5, 1, 1, 4, 1, 9, 6, 3, 6, 5, 4, 0, 3,\n",
      "        1, 0, 8, 0, 5, 3, 8, 1, 7, 6, 2, 4, 5, 7, 8, 1, 9, 6, 2, 0, 3, 6, 6, 5,\n",
      "        1, 1, 6, 1, 8, 4, 5, 1, 9, 1, 1, 8, 2, 1, 3, 9, 7, 3, 2, 1, 1, 1, 2, 9,\n",
      "        4, 6, 3, 1, 8, 0, 4, 0, 1, 5])\n",
      "torch.Size([250, 10])\n",
      "tensor([[-1., -1., -1.,  ...,  0.,  0.,  0.],\n",
      "        [-1., -1., -1.,  ...,  0.,  0.,  0.],\n",
      "        [-1., -1., -1.,  ...,  0.,  1.,  0.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ...,  0.,  0.,  0.],\n",
      "        [-1., -1., -1.,  ...,  0.,  0.,  0.],\n",
      "        [-1., -1., -1.,  ...,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "  print (data.shape)\n",
    "  print (data.flatten(1).shape)\n",
    "  print(target)\n",
    "  print(to_onehot(target, 10).shape)\n",
    "  \n",
    "  N = torch.cat([data.flatten(1), to_onehot(target, 10)], dim=1)\n",
    "  print(N)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, \n",
    "                 last_fn=None, first_fn=None, device='cpu'):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        self.flatten = flatten\n",
    "        if first_fn is not None:\n",
    "            layers.append(first_fn)\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "        else: \n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        if last_fn is not None:\n",
    "            layers.append(last_fn)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0], -1)\n",
    "        if y is not None:\n",
    "            x = torch.cat([x, y], dim=1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = FullyConnected([prior_size+10, 256, 512, 1024, 28*28], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
    "D = FullyConnected([28*28+10, 1024, 512, 256, 1], dropout=0.3, activation_fn=nn.LeakyReLU(0.2), flatten=True)\n",
    "\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = {'G': [], 'D': []}\n",
    "test_log = {'G': [], 'D': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_zeros = torch.zeros((batch_size, 1)).to(device)\n",
    "batch_ones = torch.ones((batch_size, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, G, D, log=None):\n",
    "    train_size = len(train_loader.sampler)\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        #print(label)\n",
    "        label = to_onehot(label, 10, device)\n",
    "        #print(label[0])\n",
    "        #print(label.size())\n",
    "        # train D\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        \n",
    "        z = torch.randn((batch_size, prior_size)).to(device)\n",
    "        #print(z.size())\n",
    "        #z_label = to_onehot(np.random.randint(0, 10, (batch_size)), 10, device)\n",
    "        #print(data.size())\n",
    "        #print(label.size())\n",
    "        #fake_pred = D(G(z), label)\n",
    "        #true_pred = D(data, label)\n",
    "        #a = torch.cat((z, label ),1) => 250X110\n",
    "        #G(torch.cat((z, label ),1)) =>250X784\n",
    "        #print(a.size())\n",
    "\n",
    "        \n",
    "        #fake_pred = D(torch.cat((G(torch.cat((z, label ),1)),label),1))\n",
    "        fake_pred = D(G(torch.cat((z, label ),1)), label)\n",
    "        true_pred = D(data, label)\n",
    "        #print(data.size())\n",
    "        #print(label.size())\n",
    "\n",
    "        \n",
    "        fake_loss = F.binary_cross_entropy_with_logits(fake_pred, batch_zeros)\n",
    "        true_loss = F.binary_cross_entropy_with_logits(true_pred, batch_ones)\n",
    "        \n",
    "        D_loss = 0.5*(fake_loss + true_loss)\n",
    "        \n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        \n",
    "        # train G\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        z = torch.randn((batch_size, prior_size))\n",
    "        z_label = to_onehot(np.random.randint(0, 10, (batch_size)), 10)\n",
    "        #print(z_label.size())\n",
    "        #fake_pred = D(G(z),z_label)\n",
    "        fake_pred = D(torch.cat((G(torch.cat((z, z_label ),1)),z_label),1))\n",
    "        G_loss = F.binary_cross_entropy_with_logits(fake_pred, batch_ones)\n",
    "        \n",
    "        G_loss.backward()\n",
    "        \n",
    "        G_optimizer.step()\n",
    "            \n",
    "        if batch_idx % 100 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "            losses = 'G: {:.4f}, D: {:.4f}'.format(G_loss.item(), D_loss.item())\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "        losses = 'G: {:.4f}, D: {:.4f}'.format(G_loss.item(), D_loss.item())\n",
    "        print(line + losses)\n",
    "        log['G'].append(G_loss.item())\n",
    "        log['D'].append(D_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(G, D, loader, epoch, log=None):\n",
    "    test_size = len(loader)\n",
    "    G_loss = 0.\n",
    "    D_loss = 0.\n",
    "    test_loss = {'G': 0., 'D': 0.}\n",
    "    with torch.no_grad():\n",
    "        for data, label in loader:\n",
    "            label = to_onehot(label, 10, device)\n",
    "            z = torch.randn((batch_size, prior_size))\n",
    "            #fake_pred = D(torch.cat((G(torch.cat((z, label ),1)),label),1))\n",
    "            fake_pred = D(G(torch.cat((z, label ),1)), label)\n",
    "            true_pred = D(data, label)\n",
    "        \n",
    "            fake_loss = F.binary_cross_entropy_with_logits(fake_pred, batch_zeros).item()\n",
    "            true_loss = F.binary_cross_entropy_with_logits(true_pred, batch_ones).item()\n",
    "            \n",
    "            D_loss += 0.5*(fake_loss + true_loss)\n",
    "            G_loss += F.binary_cross_entropy_with_logits(fake_pred, batch_ones).item()\n",
    "    \n",
    "    G_loss /= test_size\n",
    "    D_loss /= test_size\n",
    "\n",
    "    #fixed_gen = G(fixed_z).data.numpy().reshape(50, 1, 28, 28)\n",
    "    #plot_mnist(fixed_gen, (5, 10), True, fixed_folder + '/%03d.png' % epoch)\n",
    "    report = 'Test losses. G: {:.4f}, D: {:.4f}'.format(G_loss, D_loss)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLosses G: 1.7785, D: 0.4873\n",
      "Train Epoch: 1 [25000/50000 (50%)]\tLosses G: 1.9631, D: 0.4482\n",
      "Train Epoch: 1 [50000/50000 (100%)]\tLosses G: 1.9189, D: 0.4655\n",
      "Test losses. G: 1.8228, D: 0.4141\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLosses G: 1.6354, D: 0.4802\n",
      "Train Epoch: 2 [25000/50000 (50%)]\tLosses G: 1.7576, D: 0.4271\n",
      "Train Epoch: 2 [50000/50000 (100%)]\tLosses G: 1.7074, D: 0.3888\n",
      "Test losses. G: 1.7512, D: 0.3318\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLosses G: 2.0432, D: 0.4052\n",
      "Train Epoch: 3 [25000/50000 (50%)]\tLosses G: 1.6643, D: 0.4419\n",
      "Train Epoch: 3 [50000/50000 (100%)]\tLosses G: 1.7295, D: 0.4166\n",
      "Test losses. G: 1.7520, D: 0.3946\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLosses G: 1.7477, D: 0.4778\n",
      "Train Epoch: 4 [25000/50000 (50%)]\tLosses G: 1.7871, D: 0.4878\n",
      "Train Epoch: 4 [50000/50000 (100%)]\tLosses G: 1.8460, D: 0.4969\n",
      "Test losses. G: 1.7015, D: 0.4281\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLosses G: 1.8137, D: 0.4951\n",
      "Train Epoch: 5 [25000/50000 (50%)]\tLosses G: 1.8039, D: 0.4986\n",
      "Train Epoch: 5 [50000/50000 (100%)]\tLosses G: 1.4484, D: 0.4312\n",
      "Test losses. G: 1.4715, D: 0.3518\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLosses G: 1.6662, D: 0.4256\n",
      "Train Epoch: 6 [25000/50000 (50%)]\tLosses G: 1.7562, D: 0.4521\n",
      "Train Epoch: 6 [50000/50000 (100%)]\tLosses G: 1.5405, D: 0.5126\n",
      "Test losses. G: 1.4569, D: 0.4326\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLosses G: 1.3844, D: 0.5570\n",
      "Train Epoch: 7 [25000/50000 (50%)]\tLosses G: 1.8669, D: 0.3953\n",
      "Train Epoch: 7 [50000/50000 (100%)]\tLosses G: 2.0988, D: 0.4519\n",
      "Test losses. G: 1.9519, D: 0.3252\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLosses G: 2.4249, D: 0.4242\n",
      "Train Epoch: 8 [25000/50000 (50%)]\tLosses G: 1.5725, D: 0.5143\n",
      "Train Epoch: 8 [50000/50000 (100%)]\tLosses G: 1.5776, D: 0.4267\n",
      "Test losses. G: 1.5690, D: 0.3448\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLosses G: 1.7158, D: 0.3877\n",
      "Train Epoch: 9 [25000/50000 (50%)]\tLosses G: 1.7846, D: 0.4224\n",
      "Train Epoch: 9 [50000/50000 (100%)]\tLosses G: 1.4475, D: 0.4513\n",
      "Test losses. G: 1.5056, D: 0.4066\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLosses G: 1.3475, D: 0.4624\n",
      "Train Epoch: 10 [25000/50000 (50%)]\tLosses G: 1.5089, D: 0.4318\n",
      "Train Epoch: 10 [50000/50000 (100%)]\tLosses G: 1.6907, D: 0.4746\n",
      "Test losses. G: 1.5569, D: 0.4348\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLosses G: 1.7407, D: 0.5062\n",
      "Train Epoch: 11 [25000/50000 (50%)]\tLosses G: 1.4916, D: 0.5103\n",
      "Train Epoch: 11 [50000/50000 (100%)]\tLosses G: 1.6456, D: 0.4871\n",
      "Test losses. G: 1.5643, D: 0.4325\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLosses G: 1.6607, D: 0.4871\n",
      "Train Epoch: 12 [25000/50000 (50%)]\tLosses G: 1.7169, D: 0.3982\n",
      "Train Epoch: 12 [50000/50000 (100%)]\tLosses G: 1.6501, D: 0.3907\n",
      "Test losses. G: 1.6441, D: 0.3367\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLosses G: 1.5289, D: 0.4239\n",
      "Train Epoch: 13 [25000/50000 (50%)]\tLosses G: 2.0022, D: 0.4021\n",
      "Train Epoch: 13 [50000/50000 (100%)]\tLosses G: 1.3568, D: 0.5162\n",
      "Test losses. G: 1.3492, D: 0.4285\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLosses G: 1.4698, D: 0.5258\n",
      "Train Epoch: 14 [25000/50000 (50%)]\tLosses G: 1.3432, D: 0.4588\n",
      "Train Epoch: 14 [50000/50000 (100%)]\tLosses G: 1.3113, D: 0.4913\n",
      "Test losses. G: 1.2798, D: 0.4259\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLosses G: 1.3008, D: 0.5041\n",
      "Train Epoch: 15 [25000/50000 (50%)]\tLosses G: 1.6373, D: 0.4197\n",
      "Train Epoch: 15 [50000/50000 (100%)]\tLosses G: 1.5545, D: 0.4795\n",
      "Test losses. G: 1.4903, D: 0.4013\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLosses G: 1.5845, D: 0.4968\n",
      "Train Epoch: 16 [25000/50000 (50%)]\tLosses G: 1.7699, D: 0.4781\n",
      "Train Epoch: 16 [50000/50000 (100%)]\tLosses G: 1.6339, D: 0.4930\n",
      "Test losses. G: 1.5257, D: 0.4019\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLosses G: 1.5680, D: 0.5063\n",
      "Train Epoch: 17 [25000/50000 (50%)]\tLosses G: 1.3874, D: 0.5597\n",
      "Train Epoch: 17 [50000/50000 (100%)]\tLosses G: 1.7632, D: 0.4329\n",
      "Test losses. G: 1.6717, D: 0.3861\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLosses G: 1.6174, D: 0.4240\n",
      "Train Epoch: 18 [25000/50000 (50%)]\tLosses G: 1.3590, D: 0.5204\n",
      "Train Epoch: 18 [50000/50000 (100%)]\tLosses G: 1.4910, D: 0.5287\n",
      "Test losses. G: 1.4531, D: 0.4372\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLosses G: 1.5832, D: 0.5055\n",
      "Train Epoch: 19 [25000/50000 (50%)]\tLosses G: 1.3979, D: 0.5473\n",
      "Train Epoch: 19 [50000/50000 (100%)]\tLosses G: 1.3362, D: 0.5028\n",
      "Test losses. G: 1.3952, D: 0.4685\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLosses G: 1.4055, D: 0.5489\n",
      "Train Epoch: 20 [25000/50000 (50%)]\tLosses G: 1.5876, D: 0.4712\n",
      "Train Epoch: 20 [50000/50000 (100%)]\tLosses G: 1.4055, D: 0.4949\n",
      "Test losses. G: 1.3627, D: 0.4364\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLosses G: 1.5185, D: 0.4747\n",
      "Train Epoch: 21 [25000/50000 (50%)]\tLosses G: 1.3084, D: 0.5204\n",
      "Train Epoch: 21 [50000/50000 (100%)]\tLosses G: 1.2746, D: 0.4711\n",
      "Test losses. G: 1.3697, D: 0.4336\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLosses G: 1.2224, D: 0.5138\n",
      "Train Epoch: 22 [25000/50000 (50%)]\tLosses G: 1.6294, D: 0.4875\n",
      "Train Epoch: 22 [50000/50000 (100%)]\tLosses G: 1.5848, D: 0.4699\n",
      "Test losses. G: 1.5618, D: 0.3910\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLosses G: 1.5726, D: 0.4323\n",
      "Train Epoch: 23 [25000/50000 (50%)]\tLosses G: 1.4421, D: 0.4734\n",
      "Train Epoch: 23 [50000/50000 (100%)]\tLosses G: 1.3490, D: 0.5427\n",
      "Test losses. G: 1.3947, D: 0.4957\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLosses G: 1.4032, D: 0.5636\n",
      "Train Epoch: 24 [25000/50000 (50%)]\tLosses G: 1.2697, D: 0.4620\n",
      "Train Epoch: 24 [50000/50000 (100%)]\tLosses G: 1.4914, D: 0.4544\n",
      "Test losses. G: 1.4278, D: 0.4103\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLosses G: 1.6706, D: 0.5012\n",
      "Train Epoch: 25 [25000/50000 (50%)]\tLosses G: 1.2995, D: 0.5654\n",
      "Train Epoch: 25 [50000/50000 (100%)]\tLosses G: 1.3159, D: 0.5229\n",
      "Test losses. G: 1.3623, D: 0.4664\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLosses G: 1.2193, D: 0.4961\n",
      "Train Epoch: 26 [25000/50000 (50%)]\tLosses G: 1.2685, D: 0.4718\n",
      "Train Epoch: 26 [50000/50000 (100%)]\tLosses G: 1.3270, D: 0.4797\n",
      "Test losses. G: 1.3562, D: 0.4038\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLosses G: 1.3584, D: 0.4735\n",
      "Train Epoch: 27 [25000/50000 (50%)]\tLosses G: 1.7405, D: 0.4942\n",
      "Train Epoch: 27 [50000/50000 (100%)]\tLosses G: 1.5300, D: 0.5344\n",
      "Test losses. G: 1.4632, D: 0.4263\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLosses G: 1.4553, D: 0.5025\n",
      "Train Epoch: 28 [25000/50000 (50%)]\tLosses G: 1.3826, D: 0.4461\n",
      "Train Epoch: 28 [50000/50000 (100%)]\tLosses G: 1.3014, D: 0.5529\n",
      "Test losses. G: 1.2472, D: 0.4881\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLosses G: 1.3162, D: 0.5925\n",
      "Train Epoch: 29 [25000/50000 (50%)]\tLosses G: 1.3808, D: 0.5299\n",
      "Train Epoch: 29 [50000/50000 (100%)]\tLosses G: 1.4102, D: 0.5404\n",
      "Test losses. G: 1.3576, D: 0.4397\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLosses G: 1.2345, D: 0.5073\n",
      "Train Epoch: 30 [25000/50000 (50%)]\tLosses G: 1.4302, D: 0.4577\n",
      "Train Epoch: 30 [50000/50000 (100%)]\tLosses G: 1.3725, D: 0.5018\n",
      "Test losses. G: 1.3146, D: 0.4607\n",
      "Train Epoch: 31 [0/50000 (0%)]\tLosses G: 1.5145, D: 0.5172\n",
      "Train Epoch: 31 [25000/50000 (50%)]\tLosses G: 1.3622, D: 0.5485\n",
      "Train Epoch: 31 [50000/50000 (100%)]\tLosses G: 1.6587, D: 0.4777\n",
      "Test losses. G: 1.6590, D: 0.4349\n",
      "Train Epoch: 32 [0/50000 (0%)]\tLosses G: 1.5421, D: 0.4908\n",
      "Train Epoch: 32 [25000/50000 (50%)]\tLosses G: 1.7200, D: 0.5145\n",
      "Train Epoch: 32 [50000/50000 (100%)]\tLosses G: 1.4627, D: 0.5720\n",
      "Test losses. G: 1.3663, D: 0.5455\n",
      "Train Epoch: 33 [0/50000 (0%)]\tLosses G: 1.3635, D: 0.6029\n",
      "Train Epoch: 33 [25000/50000 (50%)]\tLosses G: 1.3841, D: 0.4693\n",
      "Train Epoch: 33 [50000/50000 (100%)]\tLosses G: 1.3662, D: 0.5408\n",
      "Test losses. G: 1.2975, D: 0.4777\n",
      "Train Epoch: 34 [0/50000 (0%)]\tLosses G: 1.2988, D: 0.5301\n",
      "Train Epoch: 34 [25000/50000 (50%)]\tLosses G: 1.3210, D: 0.5722\n",
      "Train Epoch: 34 [50000/50000 (100%)]\tLosses G: 1.1965, D: 0.5011\n",
      "Test losses. G: 1.1438, D: 0.4806\n",
      "Train Epoch: 35 [0/50000 (0%)]\tLosses G: 1.3348, D: 0.4997\n",
      "Train Epoch: 35 [25000/50000 (50%)]\tLosses G: 1.3704, D: 0.5033\n",
      "Train Epoch: 35 [50000/50000 (100%)]\tLosses G: 1.2563, D: 0.5520\n",
      "Test losses. G: 1.1848, D: 0.5039\n",
      "Train Epoch: 36 [0/50000 (0%)]\tLosses G: 1.0367, D: 0.5884\n",
      "Train Epoch: 36 [25000/50000 (50%)]\tLosses G: 1.1209, D: 0.5657\n",
      "Train Epoch: 36 [50000/50000 (100%)]\tLosses G: 1.1701, D: 0.5619\n",
      "Test losses. G: 1.1402, D: 0.4572\n",
      "Train Epoch: 37 [0/50000 (0%)]\tLosses G: 1.4007, D: 0.5520\n",
      "Train Epoch: 37 [25000/50000 (50%)]\tLosses G: 1.0973, D: 0.5877\n",
      "Train Epoch: 37 [50000/50000 (100%)]\tLosses G: 1.3616, D: 0.5109\n",
      "Test losses. G: 1.2552, D: 0.4707\n",
      "Train Epoch: 38 [0/50000 (0%)]\tLosses G: 1.3019, D: 0.5036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38 [25000/50000 (50%)]\tLosses G: 1.6369, D: 0.5060\n",
      "Train Epoch: 38 [50000/50000 (100%)]\tLosses G: 1.4312, D: 0.5161\n",
      "Test losses. G: 1.3607, D: 0.4588\n",
      "Train Epoch: 39 [0/50000 (0%)]\tLosses G: 1.5925, D: 0.5299\n",
      "Train Epoch: 39 [25000/50000 (50%)]\tLosses G: 1.3957, D: 0.5312\n",
      "Train Epoch: 39 [50000/50000 (100%)]\tLosses G: 1.3021, D: 0.5407\n",
      "Test losses. G: 1.3553, D: 0.4822\n",
      "Train Epoch: 40 [0/50000 (0%)]\tLosses G: 1.2606, D: 0.5291\n",
      "Train Epoch: 40 [25000/50000 (50%)]\tLosses G: 1.2132, D: 0.5240\n",
      "Train Epoch: 40 [50000/50000 (100%)]\tLosses G: 1.2641, D: 0.5834\n",
      "Test losses. G: 1.2599, D: 0.4952\n",
      "Train Epoch: 41 [0/50000 (0%)]\tLosses G: 1.4876, D: 0.5560\n",
      "Train Epoch: 41 [25000/50000 (50%)]\tLosses G: 1.1350, D: 0.5654\n",
      "Train Epoch: 41 [50000/50000 (100%)]\tLosses G: 1.2050, D: 0.5201\n",
      "Test losses. G: 1.2385, D: 0.4608\n",
      "Train Epoch: 42 [0/50000 (0%)]\tLosses G: 1.3409, D: 0.4834\n",
      "Train Epoch: 42 [25000/50000 (50%)]\tLosses G: 1.4594, D: 0.4833\n",
      "Train Epoch: 42 [50000/50000 (100%)]\tLosses G: 1.3305, D: 0.5962\n",
      "Test losses. G: 1.2303, D: 0.4924\n",
      "Train Epoch: 43 [0/50000 (0%)]\tLosses G: 1.2344, D: 0.5782\n",
      "Train Epoch: 43 [25000/50000 (50%)]\tLosses G: 1.2656, D: 0.5293\n",
      "Train Epoch: 43 [50000/50000 (100%)]\tLosses G: 1.3143, D: 0.5534\n",
      "Test losses. G: 1.2887, D: 0.5220\n",
      "Train Epoch: 44 [0/50000 (0%)]\tLosses G: 1.2330, D: 0.5520\n",
      "Train Epoch: 44 [25000/50000 (50%)]\tLosses G: 1.3334, D: 0.4867\n",
      "Train Epoch: 44 [50000/50000 (100%)]\tLosses G: 1.3790, D: 0.5073\n",
      "Test losses. G: 1.3133, D: 0.4765\n",
      "Train Epoch: 45 [0/50000 (0%)]\tLosses G: 1.3761, D: 0.5263\n",
      "Train Epoch: 45 [25000/50000 (50%)]\tLosses G: 1.2663, D: 0.4967\n",
      "Train Epoch: 45 [50000/50000 (100%)]\tLosses G: 1.3920, D: 0.5287\n",
      "Test losses. G: 1.4407, D: 0.4572\n",
      "Train Epoch: 46 [0/50000 (0%)]\tLosses G: 1.3472, D: 0.5372\n",
      "Train Epoch: 46 [25000/50000 (50%)]\tLosses G: 1.1942, D: 0.5433\n",
      "Train Epoch: 46 [50000/50000 (100%)]\tLosses G: 1.0777, D: 0.5552\n",
      "Test losses. G: 1.0581, D: 0.5257\n",
      "Train Epoch: 47 [0/50000 (0%)]\tLosses G: 0.9904, D: 0.5744\n",
      "Train Epoch: 47 [25000/50000 (50%)]\tLosses G: 1.3825, D: 0.5202\n",
      "Train Epoch: 47 [50000/50000 (100%)]\tLosses G: 1.4610, D: 0.5117\n",
      "Test losses. G: 1.4125, D: 0.4231\n",
      "Train Epoch: 48 [0/50000 (0%)]\tLosses G: 1.4083, D: 0.5192\n",
      "Train Epoch: 48 [25000/50000 (50%)]\tLosses G: 1.2155, D: 0.5264\n",
      "Train Epoch: 48 [50000/50000 (100%)]\tLosses G: 1.0583, D: 0.5874\n",
      "Test losses. G: 1.0405, D: 0.5388\n",
      "Train Epoch: 49 [0/50000 (0%)]\tLosses G: 1.0998, D: 0.5722\n",
      "Train Epoch: 49 [25000/50000 (50%)]\tLosses G: 1.1071, D: 0.5398\n",
      "Train Epoch: 49 [50000/50000 (100%)]\tLosses G: 1.1578, D: 0.5299\n",
      "Test losses. G: 1.2063, D: 0.4801\n",
      "Train Epoch: 50 [0/50000 (0%)]\tLosses G: 1.2032, D: 0.4994\n",
      "Train Epoch: 50 [25000/50000 (50%)]\tLosses G: 1.2450, D: 0.5644\n",
      "Train Epoch: 50 [50000/50000 (100%)]\tLosses G: 1.2364, D: 0.5452\n",
      "Test losses. G: 1.1270, D: 0.5006\n",
      "Train Epoch: 51 [0/50000 (0%)]\tLosses G: 1.1238, D: 0.5445\n",
      "Train Epoch: 51 [25000/50000 (50%)]\tLosses G: 1.2564, D: 0.5555\n",
      "Train Epoch: 51 [50000/50000 (100%)]\tLosses G: 1.1603, D: 0.5133\n",
      "Test losses. G: 1.1077, D: 0.4755\n",
      "Train Epoch: 52 [0/50000 (0%)]\tLosses G: 1.2235, D: 0.5331\n",
      "Train Epoch: 52 [25000/50000 (50%)]\tLosses G: 1.1678, D: 0.5886\n",
      "Train Epoch: 52 [50000/50000 (100%)]\tLosses G: 1.0174, D: 0.6373\n",
      "Test losses. G: 1.0062, D: 0.5543\n",
      "Train Epoch: 53 [0/50000 (0%)]\tLosses G: 1.1318, D: 0.6650\n",
      "Train Epoch: 53 [25000/50000 (50%)]\tLosses G: 1.2253, D: 0.5609\n",
      "Train Epoch: 53 [50000/50000 (100%)]\tLosses G: 1.0887, D: 0.5970\n",
      "Test losses. G: 1.1963, D: 0.5032\n",
      "Train Epoch: 54 [0/50000 (0%)]\tLosses G: 1.0957, D: 0.5585\n",
      "Train Epoch: 54 [25000/50000 (50%)]\tLosses G: 1.4038, D: 0.5050\n",
      "Train Epoch: 54 [50000/50000 (100%)]\tLosses G: 1.4928, D: 0.5379\n",
      "Test losses. G: 1.4212, D: 0.5478\n",
      "Train Epoch: 55 [0/50000 (0%)]\tLosses G: 1.2657, D: 0.5906\n",
      "Train Epoch: 55 [25000/50000 (50%)]\tLosses G: 1.2196, D: 0.5811\n",
      "Train Epoch: 55 [50000/50000 (100%)]\tLosses G: 0.9733, D: 0.6116\n",
      "Test losses. G: 1.0632, D: 0.5672\n",
      "Train Epoch: 56 [0/50000 (0%)]\tLosses G: 1.0509, D: 0.6298\n",
      "Train Epoch: 56 [25000/50000 (50%)]\tLosses G: 1.5234, D: 0.5197\n",
      "Train Epoch: 56 [50000/50000 (100%)]\tLosses G: 1.1716, D: 0.5583\n",
      "Test losses. G: 1.1289, D: 0.5045\n",
      "Train Epoch: 57 [0/50000 (0%)]\tLosses G: 1.1335, D: 0.5351\n",
      "Train Epoch: 57 [25000/50000 (50%)]\tLosses G: 1.3014, D: 0.5330\n",
      "Train Epoch: 57 [50000/50000 (100%)]\tLosses G: 1.1486, D: 0.5843\n",
      "Test losses. G: 1.0665, D: 0.5406\n",
      "Train Epoch: 58 [0/50000 (0%)]\tLosses G: 1.2098, D: 0.5636\n",
      "Train Epoch: 58 [25000/50000 (50%)]\tLosses G: 1.0153, D: 0.6036\n",
      "Train Epoch: 58 [50000/50000 (100%)]\tLosses G: 1.1203, D: 0.5364\n",
      "Test losses. G: 1.0401, D: 0.5216\n",
      "Train Epoch: 59 [0/50000 (0%)]\tLosses G: 1.0855, D: 0.5662\n",
      "Train Epoch: 59 [25000/50000 (50%)]\tLosses G: 1.3970, D: 0.5094\n",
      "Train Epoch: 59 [50000/50000 (100%)]\tLosses G: 1.1521, D: 0.5897\n",
      "Test losses. G: 1.0930, D: 0.5491\n",
      "Train Epoch: 60 [0/50000 (0%)]\tLosses G: 1.1864, D: 0.6230\n",
      "Train Epoch: 60 [25000/50000 (50%)]\tLosses G: 0.9609, D: 0.5933\n",
      "Train Epoch: 60 [50000/50000 (100%)]\tLosses G: 1.2804, D: 0.4867\n",
      "Test losses. G: 1.2326, D: 0.4896\n",
      "Train Epoch: 61 [0/50000 (0%)]\tLosses G: 1.1015, D: 0.5070\n",
      "Train Epoch: 61 [25000/50000 (50%)]\tLosses G: 1.4721, D: 0.4981\n",
      "Train Epoch: 61 [50000/50000 (100%)]\tLosses G: 1.0259, D: 0.5895\n",
      "Test losses. G: 1.0199, D: 0.5412\n",
      "Train Epoch: 62 [0/50000 (0%)]\tLosses G: 1.0153, D: 0.5475\n",
      "Train Epoch: 62 [25000/50000 (50%)]\tLosses G: 1.1871, D: 0.5779\n",
      "Train Epoch: 62 [50000/50000 (100%)]\tLosses G: 1.2529, D: 0.5372\n",
      "Test losses. G: 1.2215, D: 0.4853\n",
      "Train Epoch: 63 [0/50000 (0%)]\tLosses G: 1.1093, D: 0.5417\n",
      "Train Epoch: 63 [25000/50000 (50%)]\tLosses G: 1.2647, D: 0.5103\n",
      "Train Epoch: 63 [50000/50000 (100%)]\tLosses G: 1.2305, D: 0.5306\n",
      "Test losses. G: 1.2048, D: 0.5177\n",
      "Train Epoch: 64 [0/50000 (0%)]\tLosses G: 1.2254, D: 0.5794\n",
      "Train Epoch: 64 [25000/50000 (50%)]\tLosses G: 0.9813, D: 0.6346\n",
      "Train Epoch: 64 [50000/50000 (100%)]\tLosses G: 1.1050, D: 0.5575\n",
      "Test losses. G: 1.1169, D: 0.5101\n",
      "Train Epoch: 65 [0/50000 (0%)]\tLosses G: 1.0417, D: 0.5443\n",
      "Train Epoch: 65 [25000/50000 (50%)]\tLosses G: 1.2245, D: 0.5794\n",
      "Train Epoch: 65 [50000/50000 (100%)]\tLosses G: 1.2653, D: 0.5574\n",
      "Test losses. G: 1.2212, D: 0.5249\n",
      "Train Epoch: 66 [0/50000 (0%)]\tLosses G: 1.2574, D: 0.5289\n",
      "Train Epoch: 66 [25000/50000 (50%)]\tLosses G: 1.2392, D: 0.5561\n",
      "Train Epoch: 66 [50000/50000 (100%)]\tLosses G: 1.1947, D: 0.5451\n",
      "Test losses. G: 1.2044, D: 0.4936\n",
      "Train Epoch: 67 [0/50000 (0%)]\tLosses G: 1.2864, D: 0.5312\n",
      "Train Epoch: 67 [25000/50000 (50%)]\tLosses G: 1.2335, D: 0.5733\n",
      "Train Epoch: 67 [50000/50000 (100%)]\tLosses G: 0.9655, D: 0.5564\n",
      "Test losses. G: 1.0194, D: 0.5184\n",
      "Train Epoch: 68 [0/50000 (0%)]\tLosses G: 0.9848, D: 0.5547\n",
      "Train Epoch: 68 [25000/50000 (50%)]\tLosses G: 1.0908, D: 0.5505\n",
      "Train Epoch: 68 [50000/50000 (100%)]\tLosses G: 1.2788, D: 0.5058\n",
      "Test losses. G: 1.2880, D: 0.4871\n",
      "Train Epoch: 69 [0/50000 (0%)]\tLosses G: 1.2657, D: 0.5894\n",
      "Train Epoch: 69 [25000/50000 (50%)]\tLosses G: 1.0560, D: 0.5583\n",
      "Train Epoch: 69 [50000/50000 (100%)]\tLosses G: 0.9227, D: 0.5858\n",
      "Test losses. G: 0.9637, D: 0.5644\n",
      "Train Epoch: 70 [0/50000 (0%)]\tLosses G: 0.9157, D: 0.6157\n",
      "Train Epoch: 70 [25000/50000 (50%)]\tLosses G: 1.2675, D: 0.5201\n",
      "Train Epoch: 70 [50000/50000 (100%)]\tLosses G: 1.3737, D: 0.5720\n",
      "Test losses. G: 1.3600, D: 0.5124\n",
      "Train Epoch: 71 [0/50000 (0%)]\tLosses G: 1.3225, D: 0.5950\n",
      "Train Epoch: 71 [25000/50000 (50%)]\tLosses G: 1.3425, D: 0.5249\n",
      "Train Epoch: 71 [50000/50000 (100%)]\tLosses G: 1.4852, D: 0.5058\n",
      "Test losses. G: 1.4932, D: 0.4627\n",
      "Train Epoch: 72 [0/50000 (0%)]\tLosses G: 1.8024, D: 0.5122\n",
      "Train Epoch: 72 [25000/50000 (50%)]\tLosses G: 1.1722, D: 0.5359\n",
      "Train Epoch: 72 [50000/50000 (100%)]\tLosses G: 1.2083, D: 0.5801\n",
      "Test losses. G: 1.1640, D: 0.5601\n",
      "Train Epoch: 73 [0/50000 (0%)]\tLosses G: 1.1185, D: 0.6072\n",
      "Train Epoch: 73 [25000/50000 (50%)]\tLosses G: 0.9921, D: 0.5987\n",
      "Train Epoch: 73 [50000/50000 (100%)]\tLosses G: 1.0062, D: 0.5552\n",
      "Test losses. G: 1.0247, D: 0.5246\n",
      "Train Epoch: 74 [0/50000 (0%)]\tLosses G: 1.0638, D: 0.5384\n",
      "Train Epoch: 74 [25000/50000 (50%)]\tLosses G: 1.0716, D: 0.5693\n",
      "Train Epoch: 74 [50000/50000 (100%)]\tLosses G: 1.0857, D: 0.5674\n",
      "Test losses. G: 1.0997, D: 0.5295\n",
      "Train Epoch: 75 [0/50000 (0%)]\tLosses G: 1.0499, D: 0.5301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 75 [25000/50000 (50%)]\tLosses G: 1.0703, D: 0.5632\n",
      "Train Epoch: 75 [50000/50000 (100%)]\tLosses G: 0.9643, D: 0.5867\n",
      "Test losses. G: 0.9837, D: 0.5725\n",
      "Train Epoch: 76 [0/50000 (0%)]\tLosses G: 0.9673, D: 0.5874\n",
      "Train Epoch: 76 [25000/50000 (50%)]\tLosses G: 0.9624, D: 0.6056\n",
      "Train Epoch: 76 [50000/50000 (100%)]\tLosses G: 1.0214, D: 0.6249\n",
      "Test losses. G: 0.9975, D: 0.5825\n",
      "Train Epoch: 77 [0/50000 (0%)]\tLosses G: 1.0079, D: 0.6163\n",
      "Train Epoch: 77 [25000/50000 (50%)]\tLosses G: 1.1768, D: 0.5221\n",
      "Train Epoch: 77 [50000/50000 (100%)]\tLosses G: 1.1796, D: 0.5515\n",
      "Test losses. G: 1.1968, D: 0.4778\n",
      "Train Epoch: 78 [0/50000 (0%)]\tLosses G: 1.2381, D: 0.5578\n",
      "Train Epoch: 78 [25000/50000 (50%)]\tLosses G: 0.9671, D: 0.5577\n",
      "Train Epoch: 78 [50000/50000 (100%)]\tLosses G: 1.0267, D: 0.5929\n",
      "Test losses. G: 0.9995, D: 0.5437\n",
      "Train Epoch: 79 [0/50000 (0%)]\tLosses G: 1.0666, D: 0.6144\n",
      "Train Epoch: 79 [25000/50000 (50%)]\tLosses G: 1.2330, D: 0.5671\n",
      "Train Epoch: 79 [50000/50000 (100%)]\tLosses G: 1.1337, D: 0.5423\n",
      "Test losses. G: 1.0766, D: 0.5266\n",
      "Train Epoch: 80 [0/50000 (0%)]\tLosses G: 1.1170, D: 0.5414\n",
      "Train Epoch: 80 [25000/50000 (50%)]\tLosses G: 1.2597, D: 0.5313\n",
      "Train Epoch: 80 [50000/50000 (100%)]\tLosses G: 1.0887, D: 0.5593\n",
      "Test losses. G: 1.1002, D: 0.5174\n",
      "Train Epoch: 81 [0/50000 (0%)]\tLosses G: 1.1666, D: 0.5662\n",
      "Train Epoch: 81 [25000/50000 (50%)]\tLosses G: 1.0906, D: 0.5886\n",
      "Train Epoch: 81 [50000/50000 (100%)]\tLosses G: 1.1779, D: 0.5459\n",
      "Test losses. G: 1.1723, D: 0.4955\n",
      "Train Epoch: 82 [0/50000 (0%)]\tLosses G: 1.4024, D: 0.5641\n",
      "Train Epoch: 82 [25000/50000 (50%)]\tLosses G: 1.0311, D: 0.5755\n",
      "Train Epoch: 82 [50000/50000 (100%)]\tLosses G: 1.2140, D: 0.5810\n",
      "Test losses. G: 1.2043, D: 0.5238\n",
      "Train Epoch: 83 [0/50000 (0%)]\tLosses G: 1.1933, D: 0.5409\n",
      "Train Epoch: 83 [25000/50000 (50%)]\tLosses G: 1.1049, D: 0.5675\n",
      "Train Epoch: 83 [50000/50000 (100%)]\tLosses G: 1.2109, D: 0.5413\n",
      "Test losses. G: 1.1841, D: 0.5088\n",
      "Train Epoch: 84 [0/50000 (0%)]\tLosses G: 1.2397, D: 0.5312\n",
      "Train Epoch: 84 [25000/50000 (50%)]\tLosses G: 1.0865, D: 0.5644\n",
      "Train Epoch: 84 [50000/50000 (100%)]\tLosses G: 0.9610, D: 0.6182\n",
      "Test losses. G: 1.0046, D: 0.5622\n",
      "Train Epoch: 85 [0/50000 (0%)]\tLosses G: 1.0119, D: 0.5695\n",
      "Train Epoch: 85 [25000/50000 (50%)]\tLosses G: 1.1433, D: 0.5974\n",
      "Train Epoch: 85 [50000/50000 (100%)]\tLosses G: 1.1284, D: 0.5305\n",
      "Test losses. G: 1.1292, D: 0.5082\n",
      "Train Epoch: 86 [0/50000 (0%)]\tLosses G: 1.1268, D: 0.5223\n",
      "Train Epoch: 86 [25000/50000 (50%)]\tLosses G: 1.0733, D: 0.5739\n",
      "Train Epoch: 86 [50000/50000 (100%)]\tLosses G: 1.1137, D: 0.5106\n",
      "Test losses. G: 1.1200, D: 0.5190\n",
      "Train Epoch: 87 [0/50000 (0%)]\tLosses G: 1.3145, D: 0.5855\n",
      "Train Epoch: 87 [25000/50000 (50%)]\tLosses G: 1.2112, D: 0.5510\n",
      "Train Epoch: 87 [50000/50000 (100%)]\tLosses G: 1.0261, D: 0.5826\n",
      "Test losses. G: 1.0581, D: 0.5517\n",
      "Train Epoch: 88 [0/50000 (0%)]\tLosses G: 1.0676, D: 0.6057\n",
      "Train Epoch: 88 [25000/50000 (50%)]\tLosses G: 1.1484, D: 0.5759\n",
      "Train Epoch: 88 [50000/50000 (100%)]\tLosses G: 1.1306, D: 0.5302\n",
      "Test losses. G: 1.1935, D: 0.5329\n",
      "Train Epoch: 89 [0/50000 (0%)]\tLosses G: 1.1176, D: 0.5753\n",
      "Train Epoch: 89 [25000/50000 (50%)]\tLosses G: 1.0876, D: 0.5600\n",
      "Train Epoch: 89 [50000/50000 (100%)]\tLosses G: 1.4801, D: 0.5425\n",
      "Test losses. G: 1.4516, D: 0.5127\n",
      "Train Epoch: 90 [0/50000 (0%)]\tLosses G: 1.4090, D: 0.5138\n",
      "Train Epoch: 90 [25000/50000 (50%)]\tLosses G: 1.1665, D: 0.5522\n",
      "Train Epoch: 90 [50000/50000 (100%)]\tLosses G: 1.1884, D: 0.4942\n",
      "Test losses. G: 1.1634, D: 0.4866\n",
      "Train Epoch: 91 [0/50000 (0%)]\tLosses G: 1.1908, D: 0.4950\n",
      "Train Epoch: 91 [25000/50000 (50%)]\tLosses G: 1.0654, D: 0.5487\n",
      "Train Epoch: 91 [50000/50000 (100%)]\tLosses G: 1.0975, D: 0.5550\n",
      "Test losses. G: 1.0178, D: 0.5203\n",
      "Train Epoch: 92 [0/50000 (0%)]\tLosses G: 1.0627, D: 0.5465\n",
      "Train Epoch: 92 [25000/50000 (50%)]\tLosses G: 1.0703, D: 0.5798\n",
      "Train Epoch: 92 [50000/50000 (100%)]\tLosses G: 1.0674, D: 0.6203\n",
      "Test losses. G: 0.9722, D: 0.5828\n",
      "Train Epoch: 93 [0/50000 (0%)]\tLosses G: 0.8957, D: 0.6127\n",
      "Train Epoch: 93 [25000/50000 (50%)]\tLosses G: 1.0223, D: 0.5976\n",
      "Train Epoch: 93 [50000/50000 (100%)]\tLosses G: 1.0532, D: 0.5835\n",
      "Test losses. G: 1.0638, D: 0.5805\n",
      "Train Epoch: 94 [0/50000 (0%)]\tLosses G: 0.9992, D: 0.6253\n",
      "Train Epoch: 94 [25000/50000 (50%)]\tLosses G: 1.2167, D: 0.5253\n",
      "Train Epoch: 94 [50000/50000 (100%)]\tLosses G: 1.2904, D: 0.5673\n",
      "Test losses. G: 1.2235, D: 0.5332\n",
      "Train Epoch: 95 [0/50000 (0%)]\tLosses G: 1.2653, D: 0.5871\n",
      "Train Epoch: 95 [25000/50000 (50%)]\tLosses G: 0.9263, D: 0.6420\n",
      "Train Epoch: 95 [50000/50000 (100%)]\tLosses G: 1.1909, D: 0.5621\n",
      "Test losses. G: 1.1436, D: 0.5319\n",
      "Train Epoch: 96 [0/50000 (0%)]\tLosses G: 1.1837, D: 0.5287\n",
      "Train Epoch: 96 [25000/50000 (50%)]\tLosses G: 0.9304, D: 0.6020\n",
      "Train Epoch: 96 [50000/50000 (100%)]\tLosses G: 0.9300, D: 0.6064\n",
      "Test losses. G: 0.9716, D: 0.5661\n",
      "Train Epoch: 97 [0/50000 (0%)]\tLosses G: 0.9673, D: 0.5878\n",
      "Train Epoch: 97 [25000/50000 (50%)]\tLosses G: 1.0375, D: 0.5813\n",
      "Train Epoch: 97 [50000/50000 (100%)]\tLosses G: 1.1326, D: 0.5849\n",
      "Test losses. G: 1.0832, D: 0.5766\n",
      "Train Epoch: 98 [0/50000 (0%)]\tLosses G: 1.1000, D: 0.5685\n",
      "Train Epoch: 98 [25000/50000 (50%)]\tLosses G: 1.1316, D: 0.5327\n",
      "Train Epoch: 98 [50000/50000 (100%)]\tLosses G: 1.3538, D: 0.5666\n",
      "Test losses. G: 1.2553, D: 0.5206\n",
      "Train Epoch: 99 [0/50000 (0%)]\tLosses G: 1.4302, D: 0.5557\n",
      "Train Epoch: 99 [25000/50000 (50%)]\tLosses G: 1.3386, D: 0.5700\n",
      "Train Epoch: 99 [50000/50000 (100%)]\tLosses G: 1.0284, D: 0.5714\n",
      "Test losses. G: 1.0586, D: 0.5529\n",
      "Train Epoch: 100 [0/50000 (0%)]\tLosses G: 1.0776, D: 0.6034\n",
      "Train Epoch: 100 [25000/50000 (50%)]\tLosses G: 1.1810, D: 0.5787\n",
      "Train Epoch: 100 [50000/50000 (100%)]\tLosses G: 0.9789, D: 0.5655\n",
      "Test losses. G: 1.0475, D: 0.5484\n",
      "Train Epoch: 101 [0/50000 (0%)]\tLosses G: 1.0495, D: 0.6018\n",
      "Train Epoch: 101 [25000/50000 (50%)]\tLosses G: 1.1476, D: 0.5911\n",
      "Train Epoch: 101 [50000/50000 (100%)]\tLosses G: 1.0746, D: 0.6748\n",
      "Test losses. G: 0.9938, D: 0.5852\n",
      "Train Epoch: 102 [0/50000 (0%)]\tLosses G: 1.0574, D: 0.5897\n",
      "Train Epoch: 102 [25000/50000 (50%)]\tLosses G: 1.0736, D: 0.5589\n",
      "Train Epoch: 102 [50000/50000 (100%)]\tLosses G: 0.8849, D: 0.6414\n",
      "Test losses. G: 0.8573, D: 0.6228\n",
      "Train Epoch: 103 [0/50000 (0%)]\tLosses G: 0.7911, D: 0.6295\n",
      "Train Epoch: 103 [25000/50000 (50%)]\tLosses G: 1.0869, D: 0.5521\n",
      "Train Epoch: 103 [50000/50000 (100%)]\tLosses G: 1.0771, D: 0.6096\n",
      "Test losses. G: 1.0252, D: 0.5720\n",
      "Train Epoch: 104 [0/50000 (0%)]\tLosses G: 1.0446, D: 0.5770\n",
      "Train Epoch: 104 [25000/50000 (50%)]\tLosses G: 1.1419, D: 0.5510\n",
      "Train Epoch: 104 [50000/50000 (100%)]\tLosses G: 1.0047, D: 0.5654\n",
      "Test losses. G: 1.0461, D: 0.5578\n",
      "Train Epoch: 105 [0/50000 (0%)]\tLosses G: 1.0167, D: 0.6028\n",
      "Train Epoch: 105 [25000/50000 (50%)]\tLosses G: 1.1460, D: 0.6136\n",
      "Train Epoch: 105 [50000/50000 (100%)]\tLosses G: 1.0984, D: 0.5643\n",
      "Test losses. G: 1.0788, D: 0.5451\n",
      "Train Epoch: 106 [0/50000 (0%)]\tLosses G: 0.9839, D: 0.5920\n",
      "Train Epoch: 106 [25000/50000 (50%)]\tLosses G: 0.9010, D: 0.6828\n",
      "Train Epoch: 106 [50000/50000 (100%)]\tLosses G: 0.9433, D: 0.6072\n",
      "Test losses. G: 0.9309, D: 0.6145\n",
      "Train Epoch: 107 [0/50000 (0%)]\tLosses G: 1.0419, D: 0.6293\n",
      "Train Epoch: 107 [25000/50000 (50%)]\tLosses G: 0.8333, D: 0.5984\n",
      "Train Epoch: 107 [50000/50000 (100%)]\tLosses G: 1.0810, D: 0.5756\n",
      "Test losses. G: 1.0187, D: 0.5461\n",
      "Train Epoch: 108 [0/50000 (0%)]\tLosses G: 1.0509, D: 0.5748\n",
      "Train Epoch: 108 [25000/50000 (50%)]\tLosses G: 1.1488, D: 0.6030\n",
      "Train Epoch: 108 [50000/50000 (100%)]\tLosses G: 0.9332, D: 0.6335\n",
      "Test losses. G: 0.8996, D: 0.6026\n",
      "Train Epoch: 109 [0/50000 (0%)]\tLosses G: 0.8582, D: 0.6467\n",
      "Train Epoch: 109 [25000/50000 (50%)]\tLosses G: 0.9639, D: 0.6012\n",
      "Train Epoch: 109 [50000/50000 (100%)]\tLosses G: 1.2461, D: 0.5800\n",
      "Test losses. G: 1.2110, D: 0.5413\n",
      "Train Epoch: 110 [0/50000 (0%)]\tLosses G: 1.2280, D: 0.5742\n",
      "Train Epoch: 110 [25000/50000 (50%)]\tLosses G: 1.1446, D: 0.5388\n",
      "Train Epoch: 110 [50000/50000 (100%)]\tLosses G: 1.1744, D: 0.5919\n",
      "Test losses. G: 1.1799, D: 0.5357\n",
      "Train Epoch: 111 [0/50000 (0%)]\tLosses G: 1.1708, D: 0.5980\n",
      "Train Epoch: 111 [25000/50000 (50%)]\tLosses G: 0.9713, D: 0.6368\n",
      "Train Epoch: 111 [50000/50000 (100%)]\tLosses G: 1.0226, D: 0.6064\n",
      "Test losses. G: 1.0681, D: 0.5857\n",
      "Train Epoch: 112 [0/50000 (0%)]\tLosses G: 1.0033, D: 0.6100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 112 [25000/50000 (50%)]\tLosses G: 1.0247, D: 0.6222\n",
      "Train Epoch: 112 [50000/50000 (100%)]\tLosses G: 0.9696, D: 0.6205\n",
      "Test losses. G: 1.0087, D: 0.6034\n",
      "Train Epoch: 113 [0/50000 (0%)]\tLosses G: 0.9977, D: 0.5875\n",
      "Train Epoch: 113 [25000/50000 (50%)]\tLosses G: 0.9359, D: 0.6296\n",
      "Train Epoch: 113 [50000/50000 (100%)]\tLosses G: 1.0087, D: 0.6130\n",
      "Test losses. G: 0.9919, D: 0.6092\n",
      "Train Epoch: 114 [0/50000 (0%)]\tLosses G: 0.9259, D: 0.6266\n",
      "Train Epoch: 114 [25000/50000 (50%)]\tLosses G: 1.1268, D: 0.5846\n",
      "Train Epoch: 114 [50000/50000 (100%)]\tLosses G: 0.9095, D: 0.6596\n",
      "Test losses. G: 0.9349, D: 0.6343\n",
      "Train Epoch: 115 [0/50000 (0%)]\tLosses G: 0.9314, D: 0.6254\n",
      "Train Epoch: 115 [25000/50000 (50%)]\tLosses G: 0.9742, D: 0.6256\n",
      "Train Epoch: 115 [50000/50000 (100%)]\tLosses G: 0.9181, D: 0.6221\n",
      "Test losses. G: 0.8937, D: 0.6011\n",
      "Train Epoch: 116 [0/50000 (0%)]\tLosses G: 0.9093, D: 0.6174\n",
      "Train Epoch: 116 [25000/50000 (50%)]\tLosses G: 0.8541, D: 0.6517\n",
      "Train Epoch: 116 [50000/50000 (100%)]\tLosses G: 1.1777, D: 0.5941\n",
      "Test losses. G: 1.1156, D: 0.5644\n",
      "Train Epoch: 117 [0/50000 (0%)]\tLosses G: 1.0948, D: 0.6038\n",
      "Train Epoch: 117 [25000/50000 (50%)]\tLosses G: 1.0330, D: 0.6958\n",
      "Train Epoch: 117 [50000/50000 (100%)]\tLosses G: 0.9250, D: 0.6371\n",
      "Test losses. G: 0.9332, D: 0.6044\n",
      "Train Epoch: 118 [0/50000 (0%)]\tLosses G: 0.9920, D: 0.6492\n",
      "Train Epoch: 118 [25000/50000 (50%)]\tLosses G: 1.0581, D: 0.5711\n",
      "Train Epoch: 118 [50000/50000 (100%)]\tLosses G: 1.0299, D: 0.6197\n",
      "Test losses. G: 1.0051, D: 0.5966\n",
      "Train Epoch: 119 [0/50000 (0%)]\tLosses G: 0.8946, D: 0.6188\n",
      "Train Epoch: 119 [25000/50000 (50%)]\tLosses G: 1.1004, D: 0.6023\n",
      "Train Epoch: 119 [50000/50000 (100%)]\tLosses G: 1.1312, D: 0.5642\n",
      "Test losses. G: 1.1942, D: 0.5679\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 120):\n",
    "    G.train()\n",
    "    D.train()\n",
    "    train(epoch, G, D, train_log)\n",
    "    G.eval()\n",
    "    D.eval()\n",
    "    test(G, D, valid_loader, epoch, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 110])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_z = torch.randn((10, 100))\n",
    "fixed_z_ = torch.cat([fixed_z.flatten(1), to_onehot(torch.LongTensor(np.linspace(0, 9, 10)), 10)], dim=1)\n",
    "print(fixed_z_.shape)\n",
    "fixed_gen = G(fixed_z_).data.numpy().reshape(10, 1, 28, 28)\n",
    "plot_mnist(fixed_gen, (5, 10), True, fixed_folder + '/%03d.png' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
